# CleanAI Structured Pruning Tool - Ã–zellikler

## ğŸ¯ Temel Ã–zellikler

### 1. **Tamamen BaÄŸÄ±msÄ±z Implementation**

- âœ… Sadece PyTorch ve temel Python kÃ¼tÃ¼phaneleri
- âœ… DÄ±ÅŸ pruning kÃ¼tÃ¼phanelerine (torch-pruning vb.) baÄŸÄ±mlÄ±lÄ±k YOK
- âœ… Her ÅŸey sÄ±fÄ±rdan implement edilmiÅŸ
- âœ… Ã–ÄŸrenmek ve Ã¶zelleÅŸtirmek kolay

### 2. **Weight-Activation Hybrid Scoring**

```
importance(channel_i) = Î± Ã— ||W_i||â‚‚ + Î² Ã— ||A_i||
```

- **Weight Score**: AÄŸÄ±rlÄ±k bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (magnitude-based)
- **Activation Score**: Aktivasyon katkÄ±sÄ± (data-dependent)
- **Hibrit YaklaÅŸÄ±m**: Her iki yÃ¶ntemin avantajlarÄ±nÄ± birleÅŸtirir
- **KonfigÃ¼re Edilebilir**: Î± ve Î² oranlarÄ±nÄ± kendiniz belirleyin

### 3. **Autograd-Based Dependency Tracing**

- Otomatik computational graph oluÅŸturma
- `grad_fn` objeleri ile layer baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± tespit
- Skip connections ve residual blocks'larÄ± handle eder
- Concat, split gibi operasyonlarÄ± destekler

### 4. **True Structured Pruning**

- Mask-based deÄŸil, gerÃ§ek tensor slicing
- Model boyutu fiziksel olarak kÃ¼Ã§Ã¼lÃ¼r
- Parameter sayÄ±sÄ± ve memory gerÃ§ekten azalÄ±r
- Inference hÄ±zÄ± artar

### 5. **Flexible Pruning Strategies**

- **Global Pruning**: TÃ¼m model iÃ§in tek oran
- **Layer-Specific**: Her katman iÃ§in farklÄ± oran
- **Selective**: Belirli layer'larÄ± ignore etme
- **Iterative**: AdÄ±m adÄ±m pruning (gelecek versiyonlarda)

---

## ğŸ—ï¸ Mimari AvantajlarÄ±

### ModÃ¼ler TasarÄ±m

```
core/           â†’ Graph & Dependencies
importance/     â†’ Scoring Methods
pruner/         â†’ Pruning Functions
utils/          â†’ Helpers
```

### GeniÅŸletilebilir

- Yeni layer tipleri eklemek kolay
- Custom importance scorer yazabilirsiniz
- Kendi pruning stratejinizi implement edebilirsiniz

### Debugging Friendly

- Her adÄ±m aÃ§Ä±kÃ§a gÃ¶rÃ¼lebilir
- Dependency graph'i inceleyebilirsiniz
- Pruning history kaydedilir

---

## ğŸ”§ Desteklenen Katmanlar

| Katman               | Destek | Notlar            |
| -------------------- | ------ | ----------------- |
| Conv2d               | âœ…     | Tam destek        |
| ConvTranspose2d      | âœ…     | Tam destek        |
| Linear               | âœ…     | Tam destek        |
| BatchNorm2d/1d       | âœ…     | Tam destek        |
| LayerNorm            | âœ…     | Tam destek        |
| GroupNorm            | âœ…     | Tam destek        |
| Depthwise Conv       | âœ…     | Ã–zel handling     |
| ReLU, Pooling        | âœ…     | Pass-through      |
| Skip Connections     | âœ…     | Otomatik detect   |
| Multi-head Attention | âš ï¸     | SÄ±nÄ±rlÄ± destek    |
| RNN/LSTM             | âš ï¸     | Single-layer only |

---

## ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma

### CleanAI vs DiÄŸer Pruning KÃ¼tÃ¼phaneleri

| Ã–zellik                  | CleanAI             | torch-pruning | NVIDIA Pruning |
| ------------------------ | ------------------- | ------------- | -------------- |
| BaÄŸÄ±msÄ±zlÄ±k              | âœ… Tam              | âŒ            | âŒ             |
| Weight-Activation Hybrid | âœ…                  | âŒ            | âŒ             |
| Ã–ÄŸrenme EÄŸrisi           | âœ… Kolay            | âš ï¸ Orta       | âš ï¸ Zor         |
| Ã–zelleÅŸtirme             | âœ… Kolay            | âš ï¸ Orta       | âŒ Zor         |
| DokÃ¼mantasyon            | âœ… TÃ¼rkÃ§e+Ä°ngilizce | âœ… Ä°ngilizce  | âš ï¸ SÄ±nÄ±rlÄ±     |
| Kod Kalitesi             | âœ… Clean, Simple    | âœ… Ä°yi        | âœ… Ä°yi         |

---

## ğŸ’¡ KullanÄ±m SenaryolarÄ±

### 1. Model Compression

```python
# ResNet18: 11.7M â†’ 6.5M params (%44 azalma)
pruner = StructuredPruner(
    model=resnet18,
    pruning_ratio=0.35
)
```

### 2. Mobile/Edge Deployment

```python
# MobileNet iÃ§in agresif pruning
pruner = StructuredPruner(
    model=mobilenet,
    pruning_ratio=0.5,
    layer_pruning_ratios={
        mobilenet.features[0]: 0.2,  # Ä°lk katman koruma
        mobilenet.features[-1]: 0.6   # Son katman agresif
    }
)
```

### 3. Research & Experimentation

```python
# FarklÄ± Î±-Î² kombinasyonlarÄ±nÄ± dene
for alpha in [0.3, 0.5, 0.7]:
    beta = 1 - alpha
    importance = WeightActivationImportance(
        weight_ratio=alpha,
        activation_ratio=beta
    )
    # Test et...
```

---

## ğŸ“ EÄŸitim & Ã–ÄŸrenme

### AnlaÅŸÄ±lÄ±r Kod

- Her fonksiyon yorum satÄ±rlarÄ±yla aÃ§Ä±klanmÄ±ÅŸ
- Docstring'ler detaylÄ±
- DeÄŸiÅŸken isimleri aÃ§Ä±klayÄ±cÄ±

### Ã–rneklerle Ã–ÄŸrenme

- `quick_start.py`: 5 dakikada baÅŸla
- `example_basic.py`: AdÄ±m adÄ±m aÃ§Ä±klamalÄ±
- `example_resnet.py`: Production-ready Ã¶rnek

### Teorik Temeller

- Weight magnitude pruning (classic)
- Activation-based pruning (data-dependent)
- Hybrid scoring (best of both)

---

## ğŸš€ Performance

### HÄ±z

- Graph building: ~1-2 saniye (orta bÃ¼yÃ¼klÃ¼kte model)
- Activation collection: Veri setine baÄŸlÄ±
- Pruning execution: ~1 saniye

### Memory

- Activation cache: Efficient storage
- In-place pruning: Minimum memory overhead
- Hook cleanup: No memory leaks

### Accuracy

- Aktivasyon kullanÄ±mÄ± â†’ Daha iyi accuracy preservation
- Weight+Activation â†’ Optimal trade-off
- Fine-tuning ile %1-2 accuracy drop

---

## ğŸ”® Gelecek PlanlarÄ±

### v1.1 (YakÄ±nda)

- [ ] Iterative pruning support
- [ ] Pruning scheduler
- [ ] Auto-tuning pruning ratios

### v1.2

- [ ] Multi-head attention full support
- [ ] Quantization integration
- [ ] Knowledge distillation

### v2.0

- [ ] Dynamic pruning (training sÄ±rasÄ±nda)
- [ ] Neural architecture search integration
- [ ] Distributed pruning

---

## ğŸ“ Lisans

MIT License - Ã–zgÃ¼rce kullanÄ±n, deÄŸiÅŸtirin, paylaÅŸÄ±n!

---

## ğŸ¤ KatkÄ±da Bulunma

- Issues: Bug reports, feature requests
- Pull Requests: HoÅŸ geldiniz!
- Documentation: Her tÃ¼rlÃ¼ iyileÅŸtirme

---

**CleanAI Team** Â© 2024
